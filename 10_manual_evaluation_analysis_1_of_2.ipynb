{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5eab2e3",
   "metadata": {},
   "source": [
    "# Manual analysis of evaluation results (1 of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7663df",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "python 00_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b03fd",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62361a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30595540",
   "metadata": {},
   "source": [
    "### Display setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed1ec5",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711210b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIVATE = False\n",
    "DATA_DIR = Path(\"data\")\n",
    "TMP_DIR = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaeda79",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import override environment variables from .env file\n",
    "# or from private.env file if PRIVATE is True\n",
    "dotenv.load_dotenv('.env' if not PRIVATE else 'private.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b0273",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "def pretty_markdown(_text):\n",
    "    Console().print(Markdown(_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdede4eb",
   "metadata": {},
   "source": [
    "### Load evaluation file (jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation(_json_file):\n",
    "    \n",
    "    lines = []\n",
    "    with open(_json_file) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    line_dicts = [json.loads(line) for line in lines]\n",
    "    return pd.DataFrame(line_dicts)\n",
    "\n",
    "_df = load_evaluation(DATA_DIR / 'science-trivia__context_response_feedback_v12__evaluated.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3b4fa",
   "metadata": {},
   "source": [
    "### Check columns and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed89b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ad2e9",
   "metadata": {},
   "source": [
    "## Descriptive statistics\n",
    "\n",
    "### e.g. positive human feed back (thumbs up) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee02251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequency of thumbs_up \n",
    "def calculate_frequency(df, column, normalize=False):\n",
    "    return df[column].value_counts(normalize=normalize)\n",
    "\n",
    "calculate_frequency(_df, 'inputs.thumbs_up', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2df58",
   "metadata": {},
   "source": [
    "### e.g. cross tab evaluated features vs thumbs up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(_df['outputs.Groundedness.groundedness'], _df['inputs.thumbs_up'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007d91c",
   "metadata": {},
   "source": [
    "### Using machine learning to find relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff37cff",
   "metadata": {},
   "source": [
    "### Select features (X), remove duplicates, remove NAs, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0db48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(_df):\n",
    "    X = _df.drop(columns=[col for col in _df.columns if (col.startswith('inputs.') and col != 'inputs.thumbs_up') \\\n",
    "                          or _df[col].dtype == 'object' or col.endswith('_threshold')])\n",
    "    X.dropna(inplace=True)\n",
    "    return X\n",
    "\n",
    "# loop all columns and split their names by '.'    \n",
    "def split_column_names(df):\n",
    "    split_names = {}\n",
    "    for col in df.columns:\n",
    "        parts = col.split('.')\n",
    "        sn=parts[len(parts)-1]\n",
    "        split_names[sn] =  True\n",
    "    return split_names\n",
    "\n",
    "def remove_gpt_duplicates(df):\n",
    "    split_names=split_column_names(df)\n",
    "    torm = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        parts = col.split('.')\n",
    "        sn=parts[len(parts)-1]\n",
    "        if sn.startswith('gpt_') and f'{sn[4:]}' in split_names:\n",
    "            torm.append(col)\n",
    "    \n",
    "    return df.drop(columns=torm)\n",
    "\n",
    "X = remove_gpt_duplicates(get_X(_df))\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4faed",
   "metadata": {},
   "source": [
    "#### Set thumbs_up as label (Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.pop('inputs.thumbs_up').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e89b52",
   "metadata": {},
   "source": [
    "#### Use logistic regression to fit predictive model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "_result = f'{result.summary()}\\n\\nCoefficients:\\n{result.params}\\n\\nP-values:\\n{result.pvalues}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b12333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07df80",
   "metadata": {},
   "source": [
    "### Use LLM to explain logistic regression result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a077927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/chat/azure_chat_openai/\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv('AZURE_OPENAI_DEPLOYMENT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    temperature=0.7,\n",
    "    top_p=0,\n",
    "    max_tokens=1600,\n",
    "    timeout=None,\n",
    "    max_retries=1,\n",
    "    cache=False\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920170b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_interpretation = llm.invoke(f'Explain the following logistic regression result:\\n\\n{_result}\\n\\nWhat are the most important features and if they influence thumbs_up positively or negatively?').content\n",
    "pretty_markdown(_interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc501dd",
   "metadata": {},
   "source": [
    "### Follow up findings from logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51923d93",
   "metadata": {},
   "source": [
    "#### e.g. Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(_df['outputs.Similarity.similarity'], _df['inputs.thumbs_up'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70798f8",
   "metadata": {},
   "source": [
    "#### e.g. Positive human feedback, yet low similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aca587",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sdf = _df[(_df['outputs.Similarity.similarity'].notnull()) & (_df['outputs.Similarity.similarity'] == 2) & (_df['inputs.thumbs_up'] == 0)][['inputs.query', 'inputs.response', 'outputs.Groundedness.groundedness_reason']]\n",
    "_sdf[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcb7c4",
   "metadata": {},
   "source": [
    "#### e.g. High similarity, wet negative human feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a37850",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sdf = _df[(_df['outputs.Similarity.similarity'].notnull()) & (_df['outputs.Similarity.similarity'] == 5) & (_df['inputs.thumbs_up'] == 0)][['inputs.query', 'inputs.response', 'outputs.Groundedness.groundedness_reason']]\n",
    "_sdf[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236bbfb",
   "metadata": {},
   "source": [
    "#### e.g. Grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(_df['outputs.Groundedness.groundedness'], _df['inputs.thumbs_up'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dce69e",
   "metadata": {},
   "source": [
    "#### e.g. Positive human feedback, yet bad grounding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_sdf = _df[(_df['outputs.Groundedness.groundedness'].notnull()) & (_df['outputs.Groundedness.groundedness'] == 1) & (_df['inputs.thumbs_up'] == 1)][['inputs.query', 'inputs.response', 'outputs.Groundedness.groundedness_reason']]\n",
    "_sdf[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd62071",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_frequency(_df, 'outputs.Groundedness.groundedness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a775e2",
   "metadata": {},
   "source": [
    "### Findings, hypothesis, ideas for further development..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1dbba3",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0816c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureai_py3_12_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

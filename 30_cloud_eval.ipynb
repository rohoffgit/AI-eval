{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud executed evaluations \n",
    "For continuous evaluation during development and production "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Evaluate your Generative AI application on the cloud with Azure AI Projects SDK (preview)<br>\n",
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation\n",
    "\n",
    "Some hints from https://carlos.mendible.com/2025/02/27/custom-evaluators-with-ai-foundry/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%cmd\n",
    "#pip install azure-identity azure-ai-projects azure-ai-ml azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "PRIVATE = False\n",
    "DATA_DIR = Path(\"data\")\n",
    "TMP_DIR = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import override environment variables from .env file\n",
    "# or from private.env file if PRIVATE is True\n",
    "dotenv.load_dotenv('.env' if not PRIVATE else 'private.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config dictionaries used by Azure AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Azure AI Foundry project\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP_AI\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_AI_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "# Configuration for Azure OpenAI model\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get AI Foundry project client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Create an Azure AI Client from a connection string. Available on Azure AI project Overview page.\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.aiprojectclient?view=azure-python-preview\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=credential,\n",
    "    conn_str=os.environ.get(\"AZURE_AI_PROJECT_CONNECTION_STRING\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.aiprojectclient?view=azure-python-preview#methods\n",
    "# Upload a file to the Azure AI Foundry project. This method required azure-ai-ml to be installed.\n",
    "# Return: tuple, containing asset id and asset URI of uploaded file.\n",
    "data_id, data_url = project_client.upload_file(DATA_DIR / \"data.jsonl\")\n",
    "print(f\"Uploaded data asset id: {data_id}\")\n",
    "print(f\"Uploaded data asset url: {data_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get built-in evaluator for their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator, GroundednessEvaluator, GroundednessProEvaluator, ViolenceEvaluator\n",
    "print(f'e.g. {F1ScoreEvaluator.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get custom evaluator library ids from Azure AI Foundry\n",
    "Note: this could also be looked up in the AI Foundry Portal (Evaluation Library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure AI Foundry project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Define ml_client to register custom evaluator\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient?view=azure-python\n",
    "ml_client = MLClient(\n",
    "       subscription_id=os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "       resource_group_name=os.environ[\"AZURE_RESOURCE_GROUP_AI\"],\n",
    "       workspace_name=os.environ[\"AZURE_AI_PROJECT_NAME\"],\n",
    "       credential=credential\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper to built evaluator library id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "def get_evaluator_library_id(_evaluator: Model) -> str:\n",
    "    _ws = ml_client.workspaces.get(ml_client.workspace_name)\n",
    "    _id=f\"azureml://locations/{_ws.location}/workspaces/{_ws._workspace_id}/models/{_evaluator.name}/versions/{_evaluator.version}\"\n",
    "    print(f\"{_evaluator.name} library id: {_id}\")\n",
    "    return _id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get library ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluator = ml_client.evaluators.get(\"AnswerLenEvaluator\", label=\"latest\")\n",
    "answerLenEvaluator_libId = get_evaluator_library_id(_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluator = ml_client.evaluators.get(\"FriendlinessEvaluator\", label=\"latest\")\n",
    "friendlinessEvaluator_libId = get_evaluator_library_id(_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start evaluation in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.models.evaluation?view=azure-python-preview\n",
    "from azure.ai.projects.models import Evaluation\n",
    "\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.models.evaluatorconfiguration?view=azure-python-preview\n",
    "from azure.ai.projects.models import EvaluatorConfiguration\n",
    "\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.models.dataset?view=azure-python-preview\n",
    "from azure.ai.projects.models import Dataset\n",
    "\n",
    "# Create an evaluation\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    \n",
    "    # Note the evaluator configuration key must follow a naming convention\n",
    "    # the string must start with a letter with only alphanumeric characters \n",
    "    # and underscores. Take \"f1_score\" as example: \"f1score\" or \"f1_evaluator\" \n",
    "    # will also be acceptable, but \"f1-score-eval\" or \"1score\" will result in errors.\n",
    "    evaluators={\n",
    "        \"f1_score\": EvaluatorConfiguration(\n",
    "            id=F1ScoreEvaluator.id,\n",
    "        ),\n",
    "\n",
    "        \"groundedness\": EvaluatorConfiguration(\n",
    "            id=GroundednessEvaluator.id,\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            },\n",
    "        ),\n",
    "\n",
    "        \"groundedness_pro\": EvaluatorConfiguration(\n",
    "            id=GroundednessProEvaluator.id,\n",
    "            init_params={\n",
    "                \"azure_ai_project\": project_client.scope\n",
    "            },\n",
    "        ),\n",
    "\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=ViolenceEvaluator.id,\n",
    "            init_params={\n",
    "                \"azure_ai_project\": project_client.scope\n",
    "            },\n",
    "        ),\n",
    "        \n",
    "        \"answer_length\": EvaluatorConfiguration(\n",
    "            id=answerLenEvaluator_libId,\n",
    "            data_mapping={\n",
    "                \"answer\": \"${data.response}\"\n",
    "            },\n",
    "        ),\n",
    "        \n",
    "        \"friendliness\": EvaluatorConfiguration(\n",
    "            id=friendlinessEvaluator_libId,\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            },\n",
    "            \n",
    "            data_mapping={\n",
    "            \"response\": \"${data.response}\"\n",
    "            } \n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create evaluation\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")\n",
    "\n",
    "# Get evaluation\n",
    "get_evaluation_response = project_client.evaluations.get(evaluation_response.id)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Created evaluation, evaluation ID: \", get_evaluation_response.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI project URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureai_py3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

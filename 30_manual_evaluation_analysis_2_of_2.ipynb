{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b24ea4",
   "metadata": {},
   "source": [
    "# Manual analysis of evaluation results (2 of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5d3cf",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "python 00_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62361a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711210b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "PRIVATE = False\n",
    "DATA_DIR = Path(\"data\")\n",
    "TMP_DIR = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(_json_file):\n",
    "    \n",
    "    lines = []\n",
    "    with open(_json_file) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    line_dicts = [json.loads(line) for line in lines]\n",
    "    return pd.DataFrame(line_dicts)\n",
    "\n",
    "_df = x(TMP_DIR / 'science-trivia__context_response_feedback_v12_locally_evaluated.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed89b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147255a9",
   "metadata": {},
   "source": [
    "## Display helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd19b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e071b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns that start with 'inputs.' but is not 'inputs.thumbs_up' or is of type 'object' from _df and return X\n",
    "def get_X(_df):\n",
    "    X = _df.drop(columns=[col for col in _df.columns if (col.startswith('inputs.') and col != 'inputs.thumbs_up') \\\n",
    "                          or _df[col].dtype == 'object' or col.endswith('_threshold') or 'groundedness_pro' in col])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a296403",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_X(_df)\n",
    "X.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb054617",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0db48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop all columns and split their names by '.'    \n",
    "def split_column_names(df):\n",
    "    split_names = {}\n",
    "    for col in df.columns:\n",
    "        parts = col.split('.')\n",
    "        sn=parts[len(parts)-1]\n",
    "        split_names[sn] =  True\n",
    "    return split_names\n",
    "\n",
    "def remove_gpt_duplicates(df):\n",
    "    split_names=split_column_names(df)\n",
    "    torm = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        parts = col.split('.')\n",
    "        sn=parts[len(parts)-1]\n",
    "        if sn.startswith('gpt_') and f'{sn[4:]}' in split_names:\n",
    "            torm.append(col)\n",
    "    \n",
    "    return df.drop(columns=torm)\n",
    "\n",
    "X = remove_gpt_duplicates(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.pop('inputs.thumbs_up').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression to predict inputs.thumbs_up from the other columns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "_result = f'{result.summary()}\\n\\nCoefficients:\\n{result.params}\\n\\nP-values:\\n{result.pvalues}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b12333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a077927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/chat/azure_chat_openai/\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv('AZURE_OPENAI_DEPLOYMENT'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION'),\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    temperature=0.7,\n",
    "    top_p=0,\n",
    "    max_tokens=1600,\n",
    "    timeout=None,\n",
    "    max_retries=1,\n",
    "    cache=False\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9416b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "def pretty_markdown(_text):\n",
    "    Console().print(Markdown(_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc623829",
   "metadata": {},
   "outputs": [],
   "source": [
    "_interpretation = llm.invoke(f'Explain the following logistic regression result:\\n\\n{_result}\\n\\nWhat are the most important features and how do they influence thumbs_up?').content\n",
    "pretty_markdown(_interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab0eca",
   "metadata": {},
   "source": [
    "### Follow up findings from logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(_df['outputs.Friendliness.score'], _df['inputs.thumbs_up'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf8843",
   "metadata": {},
   "source": [
    "#### e.g. High friendliness, yet negative human feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aca587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator excepted non-similar answers to be correct\n",
    "# Not how groundedness reason helps interpretation \n",
    "_sdf = _df[(_df['outputs.Friendliness.score'].notnull()) & (_df['outputs.Friendliness.score'] == 5) & (_df['inputs.thumbs_up'] == 0)][['inputs.query', 'inputs.ground_truth', 'inputs.response', 'outputs.Friendliness.reason']]\n",
    "_sdf[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f8a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureai_py3_12_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

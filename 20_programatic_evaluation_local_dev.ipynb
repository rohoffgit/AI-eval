{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Development -- evaluators and testing evaluation\n",
    "\n",
    "Inspiration: https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Azure AI Evaluation client library for Python<br>\n",
    "https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/evaluation/azure-ai-evaluation\n",
    "\n",
    "Evaluate your Generative AI application locally with the Azure AI Evaluation SDK<br>\n",
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "python 00_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "PRIVATE = False\n",
    "DATA_DIR = Path(\"data\")\n",
    "TMP_DIR = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import override environment variables from .env file\n",
    "# or from private.env file if PRIVATE is True\n",
    "dotenv.load_dotenv('.env' if not PRIVATE else 'private.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config dictionaries used by Azure AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Azure AI Foundry project\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP_AI\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_AI_FOUNDRY_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "# Configuration for Azure OpenAI model\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.groundednessevaluator\n",
    "groundedness_eval = GroundednessEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local testing of built-in evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_query_response_data = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    context=\"The Alpine Explorer Tent is the second most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Groundedness Evaluator on a query and response pair\n",
    "groundedness_score = groundedness_eval(\n",
    "    **_query_response_data\n",
    ")\n",
    "print(groundedness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple deterministic custom evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.answer_len.answer_length import AnswerLengthEvaluator\n",
    "\n",
    "answer_length_evaluator = AnswerLengthEvaluator()\n",
    "answer_length = answer_length_evaluator(answer=\"What is the speed of light?\")\n",
    "\n",
    "print(answer_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM based custom evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your prompt-based custom evaluator\n",
    "from custom.friendliness.friend import FriendlinessEvaluator\n",
    "\n",
    "friendliness_evaluator = FriendlinessEvaluator(model_config=model_config)\n",
    "friendliness_score = friendliness_evaluator(\n",
    "    response=\"I will not apologize for my behavior!\"\n",
    ")\n",
    "friendliness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use local compute for evaluation and Azure AI Foundry for tracking results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python#functions\n",
    "result = evaluate(\n",
    "    data=DATA_DIR / \"science-trivia__context_response_feedback_v12.jsonl\",\n",
    "\n",
    "    # Specific evaluators to use\n",
    "    evaluators={\n",
    "        \"Groundedness\": groundedness_eval,\n",
    "        \"Answer_length\": answer_length_evaluator,\n",
    "        \"Friendliness\": friendliness_evaluator\n",
    "    },\n",
    "    \n",
    "    # Column mapping for each evaluator\n",
    "    # The column mapping is used to map the columns in your data to the columns expected by the evaluator\n",
    "    # Skip if using your data uses the default column names expected by the evaluators. \n",
    "    # For example, the default column names for the GroundednessEvaluator are \"query\", \"context\", and \"response\"\n",
    "    evaluator_config={\n",
    "        \"Groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }, \n",
    "        },\n",
    "        \"Answer_length\": {\n",
    "            \"column_mapping\": {\n",
    "                \"answer\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"Friendliness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        }\n",
    "\n",
    "    },\n",
    "    \n",
    "    # Provide your Azure AI project information to track your evaluation results in your Azure AI project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    \n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and Azure AI project URL\n",
    "    # output_path=TMP_DIR / \"local-eval-result.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out evaluation in AI Foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"AI project URI: \", result[\"studio_url\"])\n",
    "print(\"----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write evaluated data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import shutil\n",
    "\n",
    "_file = TMP_DIR / 'science-trivia__context_response_feedback_v12_locally_evaluated.jsonl'\n",
    "\n",
    "# use os to del _file ignore errors\n",
    "if _file.exists():\n",
    "    os.remove(_file)\n",
    "\n",
    "with open(_file, 'a') as file:\n",
    "    for item in result['rows']:\n",
    "        file.write(json.dumps(item) + '\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureai_py3_12_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

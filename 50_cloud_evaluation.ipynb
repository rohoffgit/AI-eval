{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Evaluation -- define and execute evaluations \n",
    "For continuous evaluation during development and production "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Evaluate your Generative AI application on the cloud with Azure AI Projects SDK (preview)<br>\n",
    "https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation\n",
    "\n",
    "Some hints from https://carlos.mendible.com/2025/02/27/custom-evaluators-with-ai-foundry/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "python 00_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "PRIVATE = False\n",
    "DATA_DIR = Path(\"data\")\n",
    "TMP_DIR = Path(\"tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import override environment variables from .env file\n",
    "# or from private.env file if PRIVATE is True\n",
    "dotenv.load_dotenv('.env' if not PRIVATE else 'private.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config dictionaries used by Azure AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Azure AI Foundry project\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"AZURE_RESOURCE_GROUP_AI\"),\n",
    "    \"project_name\": os.environ.get(\"AZURE_AI_FOUNDRY_PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "# Configuration for Azure OpenAI model\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"azure_openai\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get AI Foundry project client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Create an Azure AI Client from a connection string. Available on Azure AI project Overview page.\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.aiprojectclient?view=azure-python-preview\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    os.environ.get(\"AZURE_AI_FOUNDRY_PROJECT_CONNECTION_STRING\"), credential=credential)\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data asset id: /subscriptions/c11caebe-ea81-4036-9e58-ccf406d87ead/resourceGroups/mdwsrh/providers/Microsoft.MachineLearningServices/workspaces/rohoff-0016/data/85e263dd-535a-43a5-8dee-bac742a8bbe0/versions/1\n",
      "Uploaded data asset uri: azureml://subscriptions/c11caebe-ea81-4036-9e58-ccf406d87ead/resourcegroups/mdwsrh/workspaces/rohoff-0016/datastores/workspaceblobstore/paths/LocalUpload/5957c49f55771d53eecbcd9ff484475b/science-trivia__context_response_feedback_v12.jsonl\n"
     ]
    }
   ],
   "source": [
    "# TODO upload if not exists ny version and name\n",
    "\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.operations.datasetsoperations?view=azure-python-preview#azure-ai-projects-operations-datasetsoperations-upload-file\n",
    "# Upload a file to the Azure AI Foundry project. This method required azure-ai-ml to be installed.\n",
    "# Return: tuple, containing asset id and asset URI of uploaded file.\n",
    "data_id, data_uri = project_client.upload_file(DATA_DIR / \"science-trivia__context_response_feedback_v12.jsonl\")\n",
    "print(f\"Uploaded data asset id: {data_id}\")\n",
    "print(f\"Uploaded data asset uri: {data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get built-in evaluator for their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e.g. azureml://registries/azureml/models/F1Score-Evaluator/versions/3\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator, GroundednessEvaluator, GroundednessProEvaluator, ViolenceEvaluator\n",
    "print(f'e.g. {F1ScoreEvaluator.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get custom evaluator library ids from Azure AI Foundry\n",
    "Note: this could also be looked up in the AI Foundry Portal (Evaluation Library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure AI Foundry project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "\n",
    "# Define ml_client to register custom evaluator\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient?view=azure-python\n",
    "ml_client = MLClient(\n",
    "       subscription_id=os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "       resource_group_name=os.environ[\"AZURE_RESOURCE_GROUP_AI\"],\n",
    "       workspace_name=os.environ[\"AZURE_AI_FOUNDRY_PROJECT_NAME\"],\n",
    "       credential=credential\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper to built evaluator library id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "def get_evaluator_library_id(_evaluator: Model) -> str:\n",
    "    _ws = ml_client.workspaces.get(ml_client.workspace_name)\n",
    "    _id=f\"azureml://locations/{_ws.location}/workspaces/{_ws._workspace_id}/models/{_evaluator.name}/versions/{_evaluator.version}\"\n",
    "    print(f\"{_evaluator.name} library id: {_id}\")\n",
    "    return _id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get library ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnswerLenEvaluator library id: azureml://locations/swedencentral/workspaces/69465c44-88ca-4321-9101-8d4ea38db457/models/AnswerLenEvaluator/versions/2\n"
     ]
    }
   ],
   "source": [
    "_evaluator = ml_client.evaluators.get(\"AnswerLenEvaluator\", label=\"latest\")\n",
    "answerLenEvaluator_libId = get_evaluator_library_id(_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FriendlinessEvaluator library id: azureml://locations/swedencentral/workspaces/69465c44-88ca-4321-9101-8d4ea38db457/models/FriendlinessEvaluator/versions/2\n"
     ]
    }
   ],
   "source": [
    "_evaluator = ml_client.evaluators.get(\"FriendlinessEvaluator\", label=\"latest\")\n",
    "friendlinessEvaluator_libId = get_evaluator_library_id(_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start evaluation in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Created evaluation, evaluation ID:  824b9363-ae2e-4f49-865f-89c69a845dfd\n",
      "Evaluation status:  Starting\n",
      "AI project URI:  https://ai.azure.com/build/evaluation/824b9363-ae2e-4f49-865f-89c69a845dfd?wsid=/subscriptions/c11caebe-ea81-4036-9e58-ccf406d87ead/resourceGroups/mdwsrh/providers/Microsoft.MachineLearningServices/workspaces/rohoff-0016&tid=7383a4c2-32e9-4920-8a91-36a6ac3ce4d2\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.models.evaluation?view=azure-python-preview\n",
    "from azure.ai.projects.models import Evaluation\n",
    "\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.models.evaluatorconfiguration?view=azure-python-preview\n",
    "from azure.ai.projects.models import EvaluatorConfiguration\n",
    "\n",
    "# https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.models.dataset?view=azure-python-preview\n",
    "from azure.ai.projects.models import Dataset\n",
    "\n",
    "# Create an evaluation\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    \n",
    "    # Note the evaluator configuration key must follow a naming convention\n",
    "    # the string must start with a letter with only alphanumeric characters \n",
    "    # and underscores. Take \"f1_score\" as example: \"f1score\" or \"f1_evaluator\" \n",
    "    # will also be acceptable, but \"f1-score-eval\" or \"1score\" will result in errors.\n",
    "    evaluators={\n",
    "        \"f1_score\": EvaluatorConfiguration(\n",
    "            id=F1ScoreEvaluator.id,\n",
    "        ),\n",
    "\n",
    "        \"groundedness\": EvaluatorConfiguration(\n",
    "            id=GroundednessEvaluator.id,\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            },\n",
    "        ),\n",
    "\n",
    "        \"groundedness_pro\": EvaluatorConfiguration(\n",
    "            id=GroundednessProEvaluator.id,\n",
    "            init_params={\n",
    "                \"azure_ai_project\": project_client.scope\n",
    "            },\n",
    "        ),\n",
    "\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=ViolenceEvaluator.id,\n",
    "            init_params={\n",
    "                \"azure_ai_project\": project_client.scope\n",
    "            },\n",
    "        ),\n",
    "        \n",
    "        \"answer_length\": EvaluatorConfiguration(\n",
    "            id=answerLenEvaluator_libId,\n",
    "            data_mapping={\n",
    "                \"answer\": \"${data.response}\"\n",
    "            },\n",
    "        ),\n",
    "        \n",
    "        \"friendliness\": EvaluatorConfiguration(\n",
    "            id=friendlinessEvaluator_libId,\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            },\n",
    "            \n",
    "            data_mapping={\n",
    "            \"response\": \"${data.response}\"\n",
    "            } \n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create evaluation\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")\n",
    "\n",
    "# Get evaluation\n",
    "get_evaluation_response = project_client.evaluations.get(evaluation_response.id)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Created evaluation, evaluation ID: \", get_evaluation_response.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI project URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Waiting for evaluation to complete...\n",
      "Evaluation status: Completed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    _s = project_client.evaluations.get(evaluation_response.id).status\n",
    "    if _s != 'Starting' and _s != 'Running' and _s != 'Finalizing' and _s != 'Preparing' and _s != 'Queued':\n",
    "        print(f\"Evaluation status: {_s}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Waiting for evaluation to complete... {_s}\")\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download evaluation artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifact azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.ae4e3a66-650a-4f7a-a0af-5b0b00aab987 to tmp\\evaluation_results\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# https://learn.microsoft.com/fr-fr/python/api/azure-ai-ml/azure.ai.ml.operations.joboperations?view=azure-python#azure-ai-ml-operations-joboperations-download\n",
    "ml_client.jobs.download(\n",
    "    name=evaluation_response.id,\n",
    "    download_path=TMP_DIR / \"evaluation_results\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureai_py3_12_v3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
